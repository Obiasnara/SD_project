Size = table(clusters),
Centroids = aggregate(data, by = list(Cluster = clusters), FUN = mean)
)
return(profiles)
}
# Apply to CAH clusters
cah_cluster_profiles <- cluster_profile(histo_data_no_type, cutTree_3_clust)
print(cah_cluster_profiles)
library(caret)
# Enhanced performance evaluation
conf_matrix <- confusionMatrix(predictions, histo_data$TYPE)
print(conf_matrix)
# Feature importance
library(randomForest)
rf_model <- randomForest(TYPE ~ ., data = histo_data, importance = TRUE)
varImpPlot(rf_model)
```
# Analyze the relationship between clusters and original TYPE
# Example for the 3 clusters from CAH
histo_data$CAH_Cluster <- cutTree_3_clust
# Cross-tabulation of original classes and clusters
table(histo_data$TYPE, histo_data$CAH_Cluster)
# Interpretation:
# Create a summary of the clusters' characteristics based on the means of variables within each cluster
cluster_summary <- aggregate(histo_data_no_type, by = list(Cluster = histo_data$CAH_Cluster), FUN = mean)
# Print the summary for interpretation
print(cluster_summary)
# Visualize the clustering result
ggplot(histo_data, aes(x = COMP, y = CIRC, color = CAH_Cluster)) +
geom_point(alpha = 0.6) +
labs(title = "Clusters from CAH vs. Original Vehicle Types",
x = "Compactness",
y = "Circularity",
color = "Cluster")
# Visualize the clustering result
ggplot(histo_data, aes(x = COMP, y = CIRC, color = CAH_Cluster)) +
geom_point(alpha = 0.6) +
labs(title = "Clusters from CAH vs. Original Vehicle Types",
x = "Compactness",
y = "Circularity",
color = "Cluster")
# Compare with original TYPE distribution
ggplot(histo_data, aes(x = COMP, y = CIRC, color = TYPE)) +
geom_point(alpha = 0.6) +
labs(title = "Original Vehicle Types Distribution",
x = "Compactness",
y = "Circularity",
color = "Vehicle Type")
## 4. Classification supervisée
- Procédez à l'apprentissage de l'arbre de classification pour la prédiction de la variable classe en fonction des variables restantes.
```{r setup, include=TRUE}
# Prepare the data
# Get indices for each vehicle type
bus_indices <- which(histo_data$TYPE == "bus")
opel_indices <- which(histo_data$TYPE == "opel")
saab_indices <- which(histo_data$TYPE == "saab")
van_indices <- which(histo_data$TYPE == "van")
# Sample 80% of indices for each type
c <- c(
sample(bus_indices, round(0.8 * length(bus_indices))),
sample(opel_indices, round(0.8 * length(opel_indices))),
sample(saab_indices, round(0.8 * length(saab_indices))),
sample(van_indices, round(0.8 * length(van_indices)))
)
# Fit the tree
fit <- rpart(TYPE ~ ., data = histo_data, subset = c, method = "class")
# Use rpart.plot for visualization
rpart.plot(fit,
main = "Classification Tree for Vehicle Types",
extra = 106,  # Show predicted class and percentage
fallen.leaves = TRUE)
# Predict on the remaining data (test set)
predictions <- predict(fit, histo_data[-c, ], type = "class")
confusion_matrix <- table(predictions, histo_data$TYPE[-c])
# Add the TYPE column back into the dataset
histo_data$TYPE <- as.factor(histo_data$TYPE)
# Analyze the relationship between clusters and original TYPE
# Example for the 3 clusters from CAH
histo_data$CAH_Cluster <- cutTree_3_clust
# Cross-tabulation of original classes and clusters
table(histo_data$TYPE, histo_data$CAH_Cluster)
# Interpretation:
# Create a summary of the clusters' characteristics based on the means of variables within each cluster
cluster_summary <- aggregate(histo_data_no_type, by = list(Cluster = histo_data$CAH_Cluster), FUN = mean)
# Print the summary for interpretation
print(cluster_summary)
# Visualize the clustering result
ggplot(histo_data, aes(x = COMP, y = CIRC, color = CAH_Cluster)) +
geom_point(alpha = 0.6) +
labs(title = "Clusters from CAH vs. Original Vehicle Types",
x = "Compactness",
y = "Circularity",
color = "Cluster")
# Compare with original TYPE distribution
ggplot(histo_data, aes(x = COMP, y = CIRC, color = TYPE)) +
geom_point(alpha = 0.6) +
labs(title = "Original Vehicle Types Distribution",
x = "Compactness",
y = "Circularity",
color = "Vehicle Type")
## 4. Classification supervisée
- Procédez à l'apprentissage de l'arbre de classification pour la prédiction de la variable classe en fonction des variables restantes.
```{r setup, include=TRUE}
# Prepare the data
# Get indices for each vehicle type
bus_indices <- which(histo_data$TYPE == "bus")
opel_indices <- which(histo_data$TYPE == "opel")
saab_indices <- which(histo_data$TYPE == "saab")
van_indices <- which(histo_data$TYPE == "van")
# Sample 80% of indices for each type
c <- c(
sample(bus_indices, round(0.8 * length(bus_indices))),
sample(opel_indices, round(0.8 * length(opel_indices))),
sample(saab_indices, round(0.8 * length(saab_indices))),
sample(van_indices, round(0.8 * length(van_indices)))
)
# Fit the tree
fit <- rpart(TYPE ~ ., data = histo_data, subset = c, method = "class")
# Use rpart.plot for visualization
rpart.plot(fit,
main = "Classification Tree for Vehicle Types",
extra = 106,  # Show predicted class and percentage
fallen.leaves = TRUE)
# Predict on the remaining data (test set)
predictions <- predict(fit, histo_data[-c, ], type = "class")
confusion_matrix <- table(predictions, histo_data$TYPE[-c])
print("Confusion Matrix:")
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
## Enhanced version
```{r setup, include=TRUE}
library(rpart)
library(rpart.plot)
# Bootstrap Function for Classification Tree
bootstrap_classification_tree <- function(data, n_bootstraps = 100) {
# Ensure TYPE is a factor
data$TYPE <- as.factor(data$TYPE)
# Initialize storage for error rates
classification_errors <- numeric(n_bootstraps)
# Perform bootstrap iterations
for (i in 1:n_bootstraps) {
# Bootstrap sampling (with replacement)
set.seed(i)
bootstrap_indices <- sample(nrow(data), replace = TRUE)
train_data <- data[bootstrap_indices, ]
# Identify out-of-bag sample
oob_indices <- setdiff(1:nrow(data), unique(bootstrap_indices))
test_data <- data[oob_indices, ]
# Fit classification tree
fit <- rpart(TYPE ~ ., data = train_data, method = "class")
# Predict on out-of-bag sample
predictions <- predict(fit, test_data, type = "class")
# Calculate classification error
classification_errors[i] <- mean(predictions != test_data$TYPE)
}
# Return performance metrics
list(
mean_error = mean(classification_errors),
error_variance = var(classification_errors)
)
}
# Perform bootstrap analysis
performance <- bootstrap_classification_tree(histo_data)
# Perform bootstrap analysis
performance <- bootstrap_classification_tree(histo_data)
# Visualization of final tree
final_tree <- rpart(TYPE ~ ., data = histo_data, method = "class")
rpart.plot(final_tree,
main = "Classification Tree for Vehicle Types",
extra = 106,  # Show predicted class and percentage
fallen.leaves = TRUE)
# Detailed Accuracy Assessment
predictions <- predict(final_tree, type = "class")
confusion_matrix <- table(predictions, histo_data$TYPE)
confusion_matrix <- table(predictions, histo_data$TYPE)
print("Confusion Matrix:")
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Overall Accuracy:", round(accuracy * 100, 2), "%"))
combined_data <- as.data.frame(data_table)
combined_data <- as.data.frame(data_table)
root = "./statlog+vehicle+silhouettes/"
names = "a;b;c;d;e;f;g;h"
collname <- c("COMP", "CIRC", "DISTCIRC", "RADIUS", "PRAX", "MAXL", "SCAT",
"ELONG", "PRREC", "MAXREC", "SVMJAX", "SVMNAX", "SRG", "SKMIN",
"SKMAJ", "KURMIN", "KURMAJ", "HOL", "TYPE")
# Construct file paths
file_paths <- paste0(root, "xa", strsplit(names, ";")[[1]], ".dat")
file_paths
file_contents <- lapply(file_paths, readLines)
# Flatten the nested list into a single character vector
flattened_data <- unlist(file_contents)
# Split each row into individual columns based on spaces
split_data <- strsplit(flattened_data, "\\s+")
# Convert to a data frame
data_table <- do.call(rbind, split_data)
# Optionally, assign column names
colnames(data_table) <- paste0(collname)
combined_data <- as.data.frame(data_table)
# Missing values check
sum(is.na(histo_data))
histo_data
# Change the type of the columns (except the last one) to numeric
histo_data[, 1:18] <- sapply(histo_data[, 1:18], as.numeric)
# Plot setup
par(mfrow = c(5, 4), mar = c(4, 4, 2, 1))  # Adjust layout and margins
histo_data_no_type = histo_data[, 1:18]
# Plot histograms for each variable
for (col_name in colnames(histo_data_no_type)) {
hist(histo_data[[col_name]],
main = paste("Distribution de", col_name),
xlab = col_name,
col = "lightblue",
border = "black")
}
boxplot(histo_data_no_type,
main = "Boxplot des variables",
col = "lightblue",
border = "black")
# New window for boxplot
par(mfrow = c(1, 1), mar = c(4, 4, 2, 1))  # Reset layout and margins
boxplot(histo_data_no_type,
main = "Boxplot des variables",
col = "lightblue",
border = "black")
# Plot histograms for each variable
for (col_name in colnames(histo_data_no_type)) {
hist(histo_data[[col_name]],
main = paste("Distribution de", col_name),
xlab = col_name,
col = "lightblue",
border = "black")
}
# New window for boxplot
par(mfrow = c(1, 1), mar = c(4, 4, 2, 1))  # Reset layout and margins
# New window for boxplot
par(mfrow = c(5, 4), mar = c(4, 4, 2, 1))  # Adjust layout and margins
for (col_name in colnames(histo_data_no_type)) {
hist(histo_data[[col_name]],
main = paste("Distribution de", col_name),
xlab = col_name,
col = "lightblue",
border = "black")
}
# New window for boxplot
par(mfrow = c(1, 1), mar = c(4, 4, 2, 1))  # Reset layout and margins
boxplot(histo_data_no_type,
main = "Boxplot des variables",
col = "lightblue",
border = "black")
library(corrplot)
# Enhanced descriptive statistics
summary_stats <- data.frame(
Mean = sapply(histo_data_no_type, mean),
Median = sapply(histo_data_no_type, median),
StdDev = sapply(histo_data_no_type, sd),
Min = sapply(histo_data_no_type, min),
Max = sapply(histo_data_no_type, max)
)
print(summary_stats)
# Correlation Matrix
correlation_matrix <- cor(histo_data_no_type)
corrplot(correlation_matrix, method = 'color', order = 'alphabet')
library(corrplot)
library(cluster)
library(ggplot2)
root = "./statlog+vehicle+silhouettes/"
names = "a;b;c;d;e;f;g;h"
collname <- c("COMP", "CIRC", "DISTCIRC", "RADIUS", "PRAX", "MAXL", "SCAT",
"ELONG", "PRREC", "MAXREC", "SVMJAX", "SVMNAX", "SRG", "SKMIN",
"SKMAJ", "KURMIN", "KURMAJ", "HOL", "TYPE")
# Construct file paths
file_paths <- paste0(root, "xa", strsplit(names, ";")[[1]], ".dat")
file_paths
file_contents <- lapply(file_paths, readLines)
# Flatten the nested list into a single character vector
flattened_data <- unlist(file_contents)
# Split each row into individual columns based on spaces
split_data <- strsplit(flattened_data, "\\s+")
# Convert to a data frame
data_table <- do.call(rbind, split_data)
# Optionally, assign column names
colnames(data_table) <- paste0(collname)
combined_data <- as.data.frame(data_table)
histo_data <- combined_data
# Missing values check
sum(is.na(histo_data))
histo_data
# Change the type of the columns (except the last one) to numeric
histo_data[, 1:18] <- sapply(histo_data[, 1:18], as.numeric)
# Plot setup
par(mfrow = c(5, 4), mar = c(4, 4, 2, 1))  # Adjust layout and margins
histo_data_no_type = histo_data[, 1:18]
# New window for boxplot
par(mfrow = c(5, 4), mar = c(4, 4, 2, 1))  # Adjust layout and margins
for (col_name in colnames(histo_data_no_type)) {
hist(histo_data[[col_name]],
main = paste("Distribution de", col_name),
xlab = col_name,
col = "lightblue",
border = "black")
}
# New window for boxplot
par(mfrow = c(1, 1), mar = c(4, 4, 2, 1))  # Reset layout and margins
boxplot(histo_data_no_type,
main = "Boxplot des variables",
col = "lightblue",
border = "black")
# Enhanced descriptive statistics
summary_stats <- data.frame(
Mean = sapply(histo_data_no_type, mean),
Median = sapply(histo_data_no_type, median),
StdDev = sapply(histo_data_no_type, sd),
Min = sapply(histo_data_no_type, min),
Max = sapply(histo_data_no_type, max)
)
print(summary_stats)
# Correlation Matrix
correlation_matrix <- cor(histo_data_no_type)
corrplot(correlation_matrix, method = 'color', order = 'alphabet')
histo_data_no_type.km <- kmeans(histo_data_no_type, centers=3)
print(histo_data_no_type.km)
histo_data_no_type_copy <- histo_data_no_type
histo_data_no_type_copy$cluster <- as.factor(histo_data_no_type.km$cluster)
ggplot(histo_data_no_type_copy, aes(x = histo_data_no_type_copy$COMP, y = histo_data_no_type_copy$CIRC, color = cluster)) +
geom_point() +
labs(title = "K-means Clustering Results")
elbow <- function(data, max_k = 10) {
wss <- sapply(1:max_k, function(k){
kmeans(data, k)$tot.withinss
})
plot(1:max_k, wss, type = "b",
xlab = "Number of clusters",
ylab = "Within groups sum of squares")
}
elbow(histo_data_no_type)
# PAM clustering
# Sample of 100 rows
#histo_data_no_type_100_sample <- histo_data_no_type
histo_data_no_type_100_sample <- histo_data_no_type[sample(1:nrow(histo_data_no_type), 100), ]
histo_data_no_type.pam <- pam(histo_data_no_type_100_sample, k=3)
print(histo_data_no_type.pam)
plot(histo_data_no_type.pam)
silhouette_scores <- sapply(2:max_clusters, function(k) {
km <- kmeans(data, centers = k)
ss <- silhouette(km$cluster, dist(data))
mean(ss[, 3])
})
histo_data_no_type_scaled.hc <- hclust(distance_matrix, method = "ward.D2")
plot(histo_data_no_type_scaled.hc)
cutTree_3_clust <- cutree(histo_data_no_type_scaled.hc, k = 3) # We desire 3 groups
cutTree_4_clust <- cutree(histo_data_no_type_scaled.hc, k = 4) # We desire 4 groups
# Display the number of elements in each cluster
table(cutTree_3_clust)
#  1   2   3
# 376 214 162
table(cutTree_4_clust)
cent_3 <- NULL
for(k in 1:3) {
cent_3 <- rbind(cent_3, colMeans(histo_data_no_type_scaled[cutTree_3_clust == k, , drop = FALSE]))
}
cent_3
# Hierarchical clustering on centroids (centroid linkage)
cent_hc <- hclust(dist(cent_3), method = "cen")
# Plot the original and new dendrogram (based on centroids)
opar <- par(mfrow = c(1, 2))  # Set layout to display two plots
plot(histo_data_no_type_scaled.hc, labels = FALSE, hang = -1, main = "Original Dendrogram")
plot(cent_hc, labels = FALSE, hang = -1, main = "Dendrogram of Centroids")
par(opar)  # Restore the original plot layout
# Detailed cluster characterization
cluster_profile <- function(data, clusters) {
profiles <- data.frame(
Cluster = 1:length(unique(clusters)),
Size = table(clusters),
Centroids = aggregate(data, by = list(Cluster = clusters), FUN = mean)
)
return(profiles)
}
# Apply to CAH clusters
cah_cluster_profiles <- cluster_profile(histo_data_no_type, cutTree_3_clust)
print(cah_cluster_profiles)
library(caret)
# Enhanced performance evaluation
conf_matrix <- confusionMatrix(predictions, histo_data$TYPE)
print(conf_matrix)
# Feature importance
library(randomForest)
rf_model <- randomForest(TYPE ~ ., data = histo_data, importance = TRUE)
varImpPlot(rf_model)
# Add the TYPE column back into the dataset
histo_data$TYPE <- as.factor(histo_data$TYPE)
# Analyze the relationship between clusters and original TYPE
# Example for the 3 clusters from CAH
histo_data$CAH_Cluster <- cutTree_3_clust
# Cross-tabulation of original classes and clusters
table(histo_data$TYPE, histo_data$CAH_Cluster)
# Interpretation:
# Create a summary of the clusters' characteristics based on the means of variables within each cluster
cluster_summary <- aggregate(histo_data_no_type, by = list(Cluster = histo_data$CAH_Cluster), FUN = mean)
# Print the summary for interpretation
print(cluster_summary)
# Visualize the clustering result
ggplot(histo_data, aes(x = COMP, y = CIRC, color = CAH_Cluster)) +
geom_point(alpha = 0.6) +
labs(title = "Clusters from CAH vs. Original Vehicle Types",
x = "Compactness",
y = "Circularity",
color = "Cluster")
# Compare with original TYPE distribution
ggplot(histo_data, aes(x = COMP, y = CIRC, color = TYPE)) +
geom_point(alpha = 0.6) +
labs(title = "Original Vehicle Types Distribution",
x = "Compactness",
y = "Circularity",
color = "Vehicle Type")
# Prepare the data
# Get indices for each vehicle type
bus_indices <- which(histo_data$TYPE == "bus")
opel_indices <- which(histo_data$TYPE == "opel")
saab_indices <- which(histo_data$TYPE == "saab")
van_indices <- which(histo_data$TYPE == "van")
# Sample 80% of indices for each type
c <- c(
sample(bus_indices, round(0.8 * length(bus_indices))),
sample(opel_indices, round(0.8 * length(opel_indices))),
sample(saab_indices, round(0.8 * length(saab_indices))),
sample(van_indices, round(0.8 * length(van_indices)))
)
# Fit the tree
fit <- rpart(TYPE ~ ., data = histo_data, subset = c, method = "class")
# Use rpart.plot for visualization
rpart.plot(fit,
main = "Classification Tree for Vehicle Types",
extra = 106,  # Show predicted class and percentage
fallen.leaves = TRUE)
# Predict on the remaining data (test set)
predictions <- predict(fit, histo_data[-c, ], type = "class")
confusion_matrix <- table(predictions, histo_data$TYPE[-c])
print("Confusion Matrix:")
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
knitr::opts_chunk$set(echo = TRUE)
root = "./statlog+vehicle+silhouettes/"
names = "a;b;c;d;e;f;g;h"
#collname <- c("COMPACTNESS", "CIRCULARITY", "DISTANCE CIRCULARITY", "RADIUS RATIO",
#              "PR.AXIS ASPECT RATIO", "MAX.LENGTH ASPECT RATIO", "SCATTER RATIO",
#              "ELONGATEDNESS", "PR.AXIS RECTANGULARITY", "MAX.LENGTH RECTANGULARITY",
#              "SCALED VARIANCE ALONG MAJOR AXIS", "SCALED VARIANCE ALONG MINOR AXIS",
#              "SCALED RADIUS OF GYRATION", "SKEWNESS ABOUT MINOR AXIS",
#              "SKEWNESS ABOUT MAJOR AXIS", "KURTOSIS ABOUT MINOR AXIS",
#              "KURTOSIS ABOUT MAJOR AXIS", "HOLLOWS RATIO", "VEHICLE TYPE")
collname <- c("COMP", "CIRC", "DISTCIRC", "RADIUS", "PRAX", "MAXL", "SCAT",
"ELONG", "PRREC", "MAXREC", "SVMJAX", "SVMNAX", "SRG", "SKMIN",
"SKMAJ", "KURMIN", "KURMAJ", "HOL", "TYPE")
# Construct file paths
file_paths <- paste0(root, "xa", strsplit(names, ";")[[1]], ".dat")
file_paths
file_contents <- lapply(file_paths, readLines)
# Flatten the nested list into a single character vector
flattened_data <- unlist(file_contents)
# Split each row into individual columns based on spaces
split_data <- strsplit(flattened_data, "\\s+")
# Convert to a data frame
data_table <- do.call(rbind, split_data)
# Optionally, assign column names
colnames(data_table) <- paste0(collname)
combined_data <- as.data.frame(data_table)
library(corrplot)
library(cluster)
library(ggplot2)
library(caret)
library(randomForest)
root = "./statlog+vehicle+silhouettes/"
names = "a;b;c;d;e;f;g;h"
collname <- c("COMP", "CIRC", "DISTCIRC", "RADIUS", "PRAX", "MAXL", "SCAT",
"ELONG", "PRREC", "MAXREC", "SVMJAX", "SVMNAX", "SRG", "SKMIN",
"SKMAJ", "KURMIN", "KURMAJ", "HOL", "TYPE")
# Construct file paths
file_paths <- paste0(root, "xa", strsplit(names, ";")[[1]], ".dat")
file_paths
file_contents <- lapply(file_paths, readLines)
# Flatten the nested list into a single character vector
flattened_data <- unlist(file_contents)
# Split each row into individual columns based on spaces
split_data <- strsplit(flattened_data, "\\s+")
# Convert to a data frame
data_table <- do.call(rbind, split_data)
# Optionally, assign column names
colnames(data_table) <- paste0(collname)
combined_data <- as.data.frame(data_table)
histo_data <- combined_data
# Missing values check
sum(is.na(histo_data))
histo_data
# Change the type of the columns (except the last one) to numeric
histo_data[, 1:18] <- sapply(histo_data[, 1:18], as.numeric)
# Plot setup
par(mfrow = c(5, 4), mar = c(4, 4, 2, 1))  # Adjust layout and margins
histo_data_no_type = histo_data[, 1:18]
# New window for boxplot
par(mfrow = c(5, 4), mar = c(4, 4, 2, 1))  # Adjust layout and margins
for (col_name in colnames(histo_data_no_type)) {
hist(histo_data[[col_name]],
main = paste("Distribution de", col_name),
xlab = col_name,
col = "lightblue",
border = "black")
}
